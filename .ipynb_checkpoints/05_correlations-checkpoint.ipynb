{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eedda98-4f40-4208-b316-fd5cf5ead9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10d1a09-c765-42bf-9836-90dddf18a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/jupyter/neuron-hidden-inference\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: helper-functions\n",
      "  Attempting uninstall: helper-functions\n",
      "    Found existing installation: helper-functions 0.1.2\n",
      "    Uninstalling helper-functions-0.1.2:\n",
      "      Successfully uninstalled helper-functions-0.1.2\n",
      "  Running setup.py develop for helper-functions\n",
      "Successfully installed helper-functions-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eead6f4-7e12-41d7-9305-19a486586a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_default_device, to_device\n",
    "from helper_functions import evaluate_model, r2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac7ce44-218a-4704-9adb-956fc2dbf470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda \n",
      "device name: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "print(f'device: {device} \\ndevice name: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f92b57-15bb-4bc4-8880-dda44286862a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f03c81bbe10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seeds for numpy and torch\n",
    "np.random.seed(42)\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf91aa1f-aab7-4e3c-8eeb-0ad88b6f07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class correlated_spikes(object):\n",
    "    \"\"\" correlated_spikes Is the main class implemented [1]. Two basic methods\n",
    "        are implemented in this script: Cox processes and the Mixture method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C, rates, n_proc):\n",
    "        \"\"\" Constructor of correlated_spikes class.\n",
    "            Args:\n",
    "                C (mxm array)   : Is the correlation matrix (positive definite)\n",
    "                                  Its diagonal contains the firing rates for\n",
    "                                  the spike trains\n",
    "                rates (m array) : Firing rates for spike trains (only for\n",
    "                                  Mixture method)\n",
    "                n_proc          : Number of processes (dimensions of C matrix)\n",
    "            Returns:\n",
    "        \"\"\"\n",
    "        self.n_proc = n_proc\n",
    "        self.C = C\n",
    "        self.r = rates\n",
    "        self.spikes = 0\n",
    "\n",
    "    def rectify_and_decomp_corr(self):\n",
    "        \"\"\" rectify_and_decomp_corr - It rectifies and decomposes matrix C\n",
    "            using Cholesky's decomposition.\n",
    "            Args:\n",
    "            Returns:\n",
    "                L (mxm array)  : Lower triangular matrix (after Cholesky's\n",
    "                                 decomposition) with diagona filled with r^2 *\n",
    "                                 alpha (see [1])\n",
    "        \"\"\"\n",
    "        # Change diagonal with r^2\n",
    "        d = np.diag(self.C)**2\n",
    "        np.fill_diagonal(self.C, d)\n",
    "\n",
    "        # Cholesky decomposition\n",
    "        L = np.linalg.cholesky(self.C)\n",
    "\n",
    "        # Compute eigenvalues of L\n",
    "        w, v = np.linalg.eig(L)\n",
    "\n",
    "        # Compute alpha as the minimum eigenvalue with negative sign\n",
    "        alpha = -w.real.min()\n",
    "\n",
    "        # Fill the diagonal of L with r^2 * alpha\n",
    "        np.fill_diagonal(L, d * alpha)\n",
    "\n",
    "        return L\n",
    "\n",
    "    def cox_process(self, tau_c=10, time=100, dt=1):\n",
    "        \"\"\" Cox process (doubly stochastic process). It generates n_proc\n",
    "            number of correlated spike trains based on C matrix.\n",
    "            Args:\n",
    "                tau_c (float)   : Time constant (lambda, see [1])\n",
    "                time (int)      : Duration of spike trains (ms)\n",
    "                dt (float)      : Time step (discretization)\n",
    "        \"\"\"\n",
    "        ticks = int(time / dt)      # Simulation ticks\n",
    "\n",
    "        Lambda = np.exp(-dt / tau_c)\n",
    "        Sigma = np.sqrt(1 - np.exp(-2 * dt / tau_c))\n",
    "\n",
    "        # Set up rates vector\n",
    "        R = np.diag(self.C)\n",
    "        Y = np.random.normal(0, 1, (self.n_proc,))\n",
    "        S = np.zeros((ticks, self.n_proc))\n",
    "\n",
    "        # Rectify C\n",
    "        L = self.rectify_and_decomp_corr()\n",
    "\n",
    "        for t in range(ticks):\n",
    "            # Compute N independent Ornstein-Uhlberg processes\n",
    "            Y = Y * Lambda + np.random.normal(0, Sigma, (self.n_proc,))\n",
    "\n",
    "            # Compute instantaneous rates\n",
    "            X = R + np.dot(L, Y)\n",
    "\n",
    "            # Create spikes list\n",
    "            prob = np.random.uniform(0, 1, (self.n_proc,))\n",
    "            idx = (X * 0.001 * dt) > prob\n",
    "            S[t, idx] = 1\n",
    "\n",
    "        self.spikes = S.copy()\n",
    "        return S\n",
    "\n",
    "    def random_latency(self, size):\n",
    "        \"\"\" random_latency - Returns a random number based on the exponential\n",
    "            distribution.\n",
    "            Args:\n",
    "                size (int)  : Size of random numbers sample\n",
    "            Returns:\n",
    "        \"\"\"\n",
    "        return np.random.exponential(1, size=size)\n",
    "\n",
    "    def optimization_mixture(self, nu, P):\n",
    "        \"\"\" optimization_mixture - This function computes the best mixture\n",
    "            matrix P and the corresponding vector nu (firing rates) by applying\n",
    "            a gradient descent.\n",
    "            Args:\n",
    "                P (mx2n)  : Mixture matrix\n",
    "                nu (2n)   : Firing rates of independent spike trains (sources)\n",
    "            Returns:\n",
    "        \"\"\"\n",
    "        n = self.n_proc\n",
    "        P_ = P[:, :n].copy()\n",
    "        nu_ = nu[:n].copy()\n",
    "        A = np.zeros((n, n))\n",
    "\n",
    "        # Initialization of nu and P\n",
    "        nu_ = self.r.copy()\n",
    "        np.fill_diagonal(P_, 1)\n",
    "\n",
    "        # Steps\n",
    "        b = 0.01 / n\n",
    "        a = (1. / n) * b\n",
    "\n",
    "        # Iterations\n",
    "        U = np.zeros((n, ))\n",
    "        for ns in range(20000):\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i != j:\n",
    "                        x = 0.0\n",
    "                        for k in range(n):\n",
    "                            x += P_[i, k] * P_[j, k] * nu_[k]\n",
    "                        A[i, j] = x - self.C[i, j]\n",
    "                    else:\n",
    "                        A[i, j] = 0\n",
    "\n",
    "            tmp = np.dot(nu_.T, P_)\n",
    "            U[tmp >= self.r] = 1\n",
    "            U[tmp < self.r] = 0\n",
    "\n",
    "            tmp = np.dot(A, P_)\n",
    "            tmp_y = np.dot(tmp, nu_)\n",
    "            tmp_u = np.dot(U, nu_)\n",
    "\n",
    "            Y = P_ - 4 * a * tmp_y - b * tmp_u\n",
    "\n",
    "            Y[Y < 0] = 0\n",
    "            Y[Y > 1] = 1\n",
    "            P_ = Y\n",
    "\n",
    "            for i in range(n):\n",
    "                X = 0\n",
    "                for k in range(n):\n",
    "                    for l in range(n):\n",
    "                        X += P_[k, i] * P_[l, i] * A[k, l]\n",
    "\n",
    "                Y = np.dot(U, P_)\n",
    "                nu_[i] -= a * X + b * Y[i]\n",
    "                nu_[nu_ < 0] = 0\n",
    "\n",
    "        X = np.dot(P_, nu_)\n",
    "        nu[:n] = nu_.copy()\n",
    "        nu[n:] = (self.r - X)\n",
    "\n",
    "        P[:, :n] = P_\n",
    "        P[:, n:] = np.zeros((n, n))\n",
    "        np.fill_diagonal(P[:, n:], 1)\n",
    "\n",
    "        if any(nu[nu < 0]):\n",
    "            raise ValueError(\"nu contains illegal values!\")\n",
    "\n",
    "        if any(P[P < 0]) or any(P[P > 1]):\n",
    "            raise ValueError(\"P contains illegal values!\")\n",
    "\n",
    "    def offline_mixture(self, P, nu, n_src=1, n_trg=1, tau_c=10, time=1000):\n",
    "        \"\"\" offline_mixture - It's the Mixture methods implemented in [1].\n",
    "            It returns a spike list containing correlated spike trains.\n",
    "            Args:\n",
    "                P (mx2n array) : Mixture matrix (mixture probability)\n",
    "                nu (2x array)  : Firing rates of independent spike trains\n",
    "                                 (sources)\n",
    "                n_src (int)    : Number of sources spike trains\n",
    "                n_trg (int)    : Number of targets (correlated) spike trains\n",
    "                tau_c (float)  : Time constant\n",
    "                time           : Duration of correlated spike trains\n",
    "            Returns:\n",
    "                spks (array)   : Numpy structured array containing events time\n",
    "                                 and ids (id is the number of target spike\n",
    "                                 train)\n",
    "        \"\"\"\n",
    "        # Average target rate\n",
    "        r_mean = np.mean(np.dot(P, nu))\n",
    "\n",
    "        # Optimal window size\n",
    "        w_size = n_src * 1.0 / r_mean\n",
    "\n",
    "        # Window\n",
    "        w_size = int(time * 0.001)\n",
    "\n",
    "        # Number of spikes in trains\n",
    "        num_sources = np.random.poisson(nu * w_size).astype('int')\n",
    "\n",
    "        # Generate Poisson spike trains\n",
    "        source_train = []\n",
    "        for i in range(num_sources.shape[0]):\n",
    "            source_train.append(np.random.uniform(\n",
    "                0, w_size, num_sources[i]) * 1000)\n",
    "\n",
    "        spk, tm = [], []\n",
    "        for i in range(n_src):\n",
    "            for j in range(n_trg):\n",
    "                num_targets = np.random.binomial(num_sources[i], P[j, i])\n",
    "                target_train = np.random.choice(source_train[i],\n",
    "                                                size=num_targets,\n",
    "                                                replace=False)\n",
    "                spk.extend(np.ones((num_targets,)) * j)\n",
    "                tm.extend(target_train +\n",
    "                          self.random_latency(num_targets) * tau_c)\n",
    "        spk = np.array(spk, dtype='int')\n",
    "        tm = np.array(tm)\n",
    "\n",
    "        spks = np.recarray(spk.shape[0], dtype=[('t', float), ('id', int)])\n",
    "        spks['t'] = tm\n",
    "        spks['id'] = spk\n",
    "        spks.sort()\n",
    "        return spks\n",
    "\n",
    "    def extract_pyNCS_list(self, id_init=0):\n",
    "        \"\"\" extract_pyNCS_list - Extracts a spike list compatible with pyNCS\n",
    "            package.\n",
    "            Args:\n",
    "                id_init (int) : Initial id for spike trains\n",
    "            Returns:\n",
    "                tmp (array)   : A spike list that is compatible to pyNCS AER.\n",
    "        \"\"\"\n",
    "        time, id_end = self.spikes.shape\n",
    "        id_end += id_init\n",
    "        ids = range(id_init, id_end)\n",
    "        tmp = []\n",
    "        for t in range(time):\n",
    "            for i, j in enumerate(ids):\n",
    "                if self.spikes[t, i] != 0:\n",
    "                    tmp.append((j, t))\n",
    "        return np.array(tmp, dtype='int')\n",
    "\n",
    "    def raster_plot(self):\n",
    "        \"\"\" raster_plot - Draws the raster plot of already generated spike\n",
    "            trains.\n",
    "            Args:\n",
    "            Returns:\n",
    "        \"\"\"\n",
    "        self.spikes[self.spikes == 0] = np.nan\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        for i in range(self.n_proc):\n",
    "            ax.plot(self.spikes[:, i] + i, '|k', ms=20, mew=1)\n",
    "        ax.set_ylim([-.5, self.n_proc + 1])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a4eb0-8e7b-4344-ae0a-a9b8b2886f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
